{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51884680",
   "metadata": {},
   "source": [
    "<h3>Language modelling</h3>\n",
    "<p>Konrad Przewłoka</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c07f6",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768663e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KPR\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "#!pip install transformers\n",
    "#!pip install sacremoses\n",
    "from pprint import pprint\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5d277",
   "metadata": {},
   "source": [
    "<h4>Loading Twitter TwHIN-BERT model<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "125ed0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tw = transformers.AutoTokenizer.from_pretrained(\"Twitter/twhin-bert-base\")\n",
    "model_tw = transformers.AutoModelForMaskedLM.from_pretrained(\"Twitter/twhin-bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710aed7",
   "metadata": {},
   "source": [
    "<h4>Loading XLM-RoBERTa large model<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eefa873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_xlm = transformers.AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "model_xlm = transformers.AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a457939",
   "metadata": {},
   "source": [
    "<h4>Loading polish BERT cased model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08d09739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert_cased = transformers.AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "\n",
    "model_bert_cased = transformers.AutoModelForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6c911",
   "metadata": {},
   "source": [
    "<h4>Utility array of models</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c39a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {'name':\"TwHIN-BERT\", 'model':model_tw,'tokenizer':tokenizer_tw},\n",
    "    {'name':\"xlm roberta large\", 'model':model_xlm,'tokenizer':tokenizer_xlm},\n",
    "    {'name':\"Polish BERT cased\", 'model':model_bert_cased,'tokenizer':tokenizer_bert_cased}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4c19a",
   "metadata": {},
   "source": [
    "<h4>Polish cases checking</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52e5ccac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwHIN-BERT\n",
      "Warszawa to największe [MASK].\n",
      "Warszawa to największe miasto. score:0.736761212348938\n",
      "Warszawa to największe miasta. score:0.05466261878609657\n",
      "Warszawa to największe miejsce. score:0.03227910026907921\n",
      "Marek bardzo długo szukał [MASK].\n",
      "Marek bardzo długo szukał pomocy. score:0.10109560191631317\n",
      "Marek bardzo długo szukał syna. score:0.03973833844065666\n",
      "Marek bardzo długo szukał klubu. score:0.024326728656888008\n",
      "Prezenty podobały się [MASK].\n",
      "Prezenty podobały się mi. score:0.3716319799423218\n",
      "Prezenty podobały się najbardziej. score:0.0731700211763382\n",
      "Prezenty podobały się bardzo. score:0.0708843320608139\n",
      "Po wejściu na dach można obejrzeć [MASK].\n",
      "Po wejściu na dach można obejrzeć mecz. score:0.13820107281208038\n",
      "Po wejściu na dach można obejrzeć telewizor. score:0.04552201181650162\n",
      "Po wejściu na dach można obejrzeć film. score:0.04437772184610367\n",
      "Tomasz postanowił pojechać do domu z [MASK].\n",
      "Tomasz postanowił pojechać do domu z pracy. score:0.07852048426866531\n",
      "Tomasz postanowił pojechać do domu z dziećmi. score:0.07159660756587982\n",
      "Tomasz postanowił pojechać do domu z domu. score:0.0685223937034607\n",
      "Anna od dawna marzyła o [MASK].\n",
      "Anna od dawna marzyła o mnie. score:0.6754151582717896\n",
      "Anna od dawna marzyła o ciebie. score:0.03822559118270874\n",
      "Anna od dawna marzyła o siebie. score:0.020158864557743073\n",
      "Witaj [MASK]!\n",
      "Witaj cie! score:0.8057978749275208\n",
      "Witaj wszystkim! score:0.008291136473417282\n",
      "Witaj Państwu! score:0.007974086329340935\n",
      "xlm roberta large\n",
      "Warszawa to największe [MASK].\n",
      "Warszawa to największe centrum. score:0.18798306584358215\n",
      "Warszawa to największe mesto. score:0.16173779964447021\n",
      "Warszawa to największe miasto. score:0.15064065158367157\n",
      "Marek bardzo długo szukał [MASK].\n",
      "Marek bardzo długo szukał pracy. score:0.269313246011734\n",
      "Marek bardzo długo szukał odpowiedzi. score:0.060216739773750305\n",
      "Marek bardzo długo szukał firmy. score:0.04079227149486542\n",
      "Prezenty podobały się [MASK].\n",
      "Prezenty podobały się wszystkim. score:0.2968215048313141\n",
      "Prezenty podobały się bardzo. score:0.08829494565725327\n",
      "Prezenty podobały się najbardziej. score:0.07234187424182892\n",
      "Po wejściu na dach można obejrzeć [MASK].\n",
      "Po wejściu na dach można obejrzeć zdjęcia. score:0.03637628257274628\n",
      "Po wejściu na dach można obejrzeć budynek. score:0.02849288284778595\n",
      "Po wejściu na dach można obejrzeć samolot. score:0.026656435802578926\n",
      "Tomasz postanowił pojechać do domu z [MASK].\n",
      "Tomasz postanowił pojechać do domu z dzieckiem. score:0.22757956385612488\n",
      "Tomasz postanowił pojechać do domu z dziećmi. score:0.2055915892124176\n",
      "Tomasz postanowił pojechać do domu z imą. score:0.040055107325315475\n",
      "Anna od dawna marzyła o [MASK].\n",
      "Anna od dawna marzyła o miłości. score:0.28798311948776245\n",
      "Anna od dawna marzyła o nim. score:0.1556197851896286\n",
      "Anna od dawna marzyła o tym. score:0.07239670306444168\n",
      "Witaj [MASK]!\n",
      "Witaj cie! score:0.2715114653110504\n",
      "Witaj serdecznie! score:0.253441721200943\n",
      "Witaj my! score:0.05070177838206291\n",
      "Polish BERT cased\n",
      "Warszawa to największe [MASK].\n",
      "Warszawa to największe miasto. score:0.955305814743042\n",
      "Warszawa to największe województwo. score:0.006508117541670799\n",
      "Warszawa to największe lotnisko. score:0.005074853543192148\n",
      "Marek bardzo długo szukał [MASK].\n",
      "Marek bardzo długo szukał pracy. score:0.12061718851327896\n",
      "Marek bardzo długo szukał pomocy. score:0.09713589400053024\n",
      "Marek bardzo długo szukał pieniędzy. score:0.051099181175231934\n",
      "Prezenty podobały się [MASK].\n",
      "Prezenty podobały się wszystkim. score:0.19010838866233826\n",
      "Prezenty podobały się każdemu. score:0.10949026048183441\n",
      "Prezenty podobały się publiczności. score:0.06483205407857895\n",
      "Po wejściu na dach można obejrzeć [MASK].\n",
      "Po wejściu na dach można obejrzeć film. score:0.10973821580410004\n",
      "Po wejściu na dach można obejrzeć wystawę. score:0.0709657222032547\n",
      "Po wejściu na dach można obejrzeć muzeum. score:0.05612560361623764\n",
      "Tomasz postanowił pojechać do domu z [MASK].\n",
      "Tomasz postanowił pojechać do domu z rodzicami. score:0.13696953654289246\n",
      "Tomasz postanowił pojechać do domu z dziećmi. score:0.11282128840684891\n",
      "Tomasz postanowił pojechać do domu z rodziną. score:0.11279568076133728\n",
      "Anna od dawna marzyła o [MASK].\n",
      "Anna od dawna marzyła o ślubie. score:0.22229818999767303\n",
      "Anna od dawna marzyła o małżeństwie. score:0.0722438246011734\n",
      "Anna od dawna marzyła o dziecku. score:0.04007384181022644\n",
      "Witaj [MASK]!\n",
      "Witaj kochanie! score:0.10115629434585571\n",
      "Witaj ponownie! score:0.03170481696724892\n",
      "Witaj tutaj! score:0.017152434214949608\n"
     ]
    }
   ],
   "source": [
    "def unmask_sentece(model,sentence,num_tokens):\n",
    "    sentence = sentence.replace('[MASK]',model['tokenizer'].mask_token)\n",
    "    generator = transformers.pipeline(task=\"fill-mask\", model=model['model'], tokenizer=model['tokenizer'],top_k=num_tokens)\n",
    "    top_tokens = generator(sentence)\n",
    "    #print(top_tokens)\n",
    "    return top_tokens\n",
    "\n",
    "example_sentences=[\n",
    "    \"Warszawa to największe [MASK].\",\n",
    "    \"Marek bardzo długo szukał [MASK].\",\n",
    "    \"Prezenty podobały się [MASK].\",\n",
    "    \"Po wejściu na dach można obejrzeć [MASK].\",\n",
    "    \"Tomasz postanowił pojechać do domu z [MASK].\",\n",
    "    \"Anna od dawna marzyła o [MASK].\",\n",
    "    \"Witaj [MASK]!\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    print(model['name'])\n",
    "    for sentence in example_sentences:\n",
    "        print(sentence)\n",
    "        for token in unmask_sentece(model, sentence,3):\n",
    "            print(sentence.replace(\"[MASK]\",token['token_str'])+\" score:\"+str(token['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c2613",
   "metadata": {},
   "source": [
    "<h4>Testing long-range relationships</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d6d5369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwHIN-BERT\n",
      "Anna wybrała się do sklepu, gdzie [MASK] mleko.\n",
      "Anna wybrała się do sklepu, gdzie jest mleko. score:0.5127535462379456\n",
      "Anna wybrała się do sklepu, gdzie było mleko. score:0.1643742471933365\n",
      "Marek wrócił do domu i natychmiast [MASK] na kanapie.\n",
      "Marek wrócił do domu i natychmiast jest na kanapie. score:0.19211305677890778\n",
      "Marek wrócił do domu i natychmiast wraca na kanapie. score:0.09422203153371811\n",
      "To drzewo [MASK] 50 lat zanim spłonęło.\n",
      "To drzewo . 50 lat zanim spłonęło. score:0.12717671692371368\n",
      "To drzewo było 50 lat zanim spłonęło. score:0.07814445346593857\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go [MASK].\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go kocha. score:0.28028708696365356\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go lubi. score:0.16457785665988922\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo [MASK] lubił.\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go lubił. score:0.8882428407669067\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo ich lubił. score:0.021871838718652725\n",
      "xlm roberta large\n",
      "Anna wybrała się do sklepu, gdzie [MASK] mleko.\n",
      "Anna wybrała się do sklepu, gdzie kupuje mleko. score:0.32525697350502014\n",
      "Anna wybrała się do sklepu, gdzie kupił mleko. score:0.3139776885509491\n",
      "Marek wrócił do domu i natychmiast [MASK] na kanapie.\n",
      "Marek wrócił do domu i natychmiast spal na kanapie. score:0.4577842056751251\n",
      "Marek wrócił do domu i natychmiast siedzi na kanapie. score:0.1875019520521164\n",
      "To drzewo [MASK] 50 lat zanim spłonęło.\n",
      "To drzewo miało 50 lat zanim spłonęło. score:0.6609513759613037\n",
      "To drzewo było 50 lat zanim spłonęło. score:0.052056264132261276\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go [MASK].\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go lubi. score:0.7305657267570496\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go kocha. score:0.05502576753497124\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo [MASK] lubił.\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go lubił. score:0.9484279155731201\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo Go lubił. score:0.018286358565092087\n",
      "Polish BERT cased\n",
      "Anna wybrała się do sklepu, gdzie [MASK] mleko.\n",
      "Anna wybrała się do sklepu, gdzie kupiła mleko. score:0.4826406240463257\n",
      "Anna wybrała się do sklepu, gdzie kupić mleko. score:0.07256893813610077\n",
      "Marek wrócił do domu i natychmiast [MASK] na kanapie.\n",
      "Marek wrócił do domu i natychmiast spał na kanapie. score:0.37920308113098145\n",
      "Marek wrócił do domu i natychmiast usiadł na kanapie. score:0.20137248933315277\n",
      "To drzewo [MASK] 50 lat zanim spłonęło.\n",
      "To drzewo miało 50 lat zanim spłonęło. score:0.5705937147140503\n",
      "To drzewo żyło 50 lat zanim spłonęło. score:0.07991091161966324\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go [MASK].\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go kocha. score:0.5298860669136047\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go lubi. score:0.21066278219223022\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo [MASK] lubił.\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo go lubił. score:0.9607927799224854\n",
      "Tomek postanowił odwiedzić Marka, ponieważ bardzo ją lubił. score:0.028718160465359688\n"
     ]
    }
   ],
   "source": [
    "example_sentences_relationships=[\n",
    "    \"Anna wybrała się do sklepu, gdzie [MASK] mleko.\",\n",
    "    \"Marek wrócił do domu i natychmiast [MASK] na kanapie.\",\n",
    "    \"To drzewo [MASK] 50 lat zanim spłonęło.\",\n",
    "    \"Tomek postanowił odwiedzić Marka, ponieważ bardzo go [MASK].\",\n",
    "    \"Tomek postanowił odwiedzić Marka, ponieważ bardzo [MASK] lubił.\"    \n",
    "]\n",
    "for model in models:\n",
    "    print(model['name'])\n",
    "    for sentence in example_sentences_relationships:\n",
    "        print(sentence)\n",
    "        for token in unmask_sentece(model, sentence,2):\n",
    "            print(sentence.replace(\"[MASK]\",token['token_str'])+\" score:\"+str(token['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642601f3",
   "metadata": {},
   "source": [
    "<h4>Checking real-world knowledge</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61146e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwHIN-BERT\n",
      "Tomek studiuje polonistykę na [MASK].\n",
      "Tomek studiuje polonistykę na uczelni. score:0.10560335218906403\n",
      "Tomek studiuje polonistykę na żywo. score:0.08208702504634857\n",
      "[MASK] wschodzi o ósmej i zachodzi około szesnastej.\n",
      "Czas wschodzi o ósmej i zachodzi około szesnastej. score:0.0806540995836258\n",
      "Dzień wschodzi o ósmej i zachodzi około szesnastej. score:0.055803991854190826\n",
      "Nocami wpatrywał się w [MASK] Saturna.\n",
      "Nocami wpatrywał się w twarz Saturna. score:0.2190801501274109\n",
      "Nocami wpatrywał się w oczy Saturna. score:0.05704287439584732\n",
      "[MASK] leży pomiędzy Chinami a Rosją.\n",
      "Polska leży pomiędzy Chinami a Rosją. score:0.2131042778491974\n",
      "Putin leży pomiędzy Chinami a Rosją. score:0.04738715663552284\n",
      "xlm roberta large\n",
      "Tomek studiuje polonistykę na [MASK].\n",
      "Tomek studiuje polonistykę na uczelni. score:0.11222496628761292\n",
      "Tomek studiuje polonistykę na UM. score:0.068116195499897\n",
      "[MASK] wschodzi o ósmej i zachodzi około szesnastej.\n",
      "słońce wschodzi o ósmej i zachodzi około szesnastej. score:0.6452454328536987\n",
      "Luna wschodzi o ósmej i zachodzi około szesnastej. score:0.07942677289247513\n",
      "Nocami wpatrywał się w [MASK] Saturna.\n",
      "Nocami wpatrywał się w stronę Saturna. score:0.23705199360847473\n",
      "Nocami wpatrywał się w światło Saturna. score:0.11733760684728622\n",
      "[MASK] leży pomiędzy Chinami a Rosją.\n",
      "Polska leży pomiędzy Chinami a Rosją. score:0.2021137773990631\n",
      "Ukraina leży pomiędzy Chinami a Rosją. score:0.10854209959506989\n",
      "Polish BERT cased\n",
      "Tomek studiuje polonistykę na [MASK].\n",
      "Tomek studiuje polonistykę na UW. score:0.33964693546295166\n",
      "Tomek studiuje polonistykę na UJ. score:0.13888446986675262\n",
      "[MASK] wschodzi o ósmej i zachodzi około szesnastej.\n",
      "Słońce wschodzi o ósmej i zachodzi około szesnastej. score:0.9800357222557068\n",
      "Księżyc wschodzi o ósmej i zachodzi około szesnastej. score:0.0019672454800456762\n",
      "Nocami wpatrywał się w [MASK] Saturna.\n",
      "Nocami wpatrywał się w niebo Saturna. score:0.1147015243768692\n",
      "Nocami wpatrywał się w gwiazdy Saturna. score:0.11319441348314285\n",
      "[MASK] leży pomiędzy Chinami a Rosją.\n",
      "Polska leży pomiędzy Chinami a Rosją. score:0.10638536512851715\n",
      "Kraj leży pomiędzy Chinami a Rosją. score:0.08173621445894241\n"
     ]
    }
   ],
   "source": [
    "example_sentences_relationships=[\n",
    "    \"Tomek studiuje polonistykę na [MASK].\",\n",
    "    \"[MASK] wschodzi o ósmej i zachodzi około szesnastej.\",\n",
    "    \"Nocami wpatrywał się w [MASK] Saturna.\",\n",
    "    \"[MASK] leży pomiędzy Chinami a Rosją.\"\n",
    "]\n",
    "for model in models:\n",
    "    print(model['name'])\n",
    "    for sentence in example_sentences_relationships:\n",
    "        print(sentence)\n",
    "        for token in unmask_sentece(model, sentence,2):\n",
    "            print(sentence.replace(\"[MASK]\",token['token_str'])+\" score:\"+str(token['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e564f4",
   "metadata": {},
   "source": [
    "<h4>Answers</h4>\n",
    "<h5>I</h5>\n",
    "<p>The model to produce best results was Polish BERT cased, available at: https://huggingface.co/dkleczek/bert-base-polish-cased-v1</p>\n",
    "<h5>II</h5>\n",
    "<p>All of tested models seem to capture at least a rudimentary level of polish grammar</p>\n",
    "<h5>III</h5>\n",
    "<p>All models display capability in terms of capturing long-distance relationships. That being said, the TwHIN-BERT model results are of lower quality compared to others. The polish BERT cased model seems to offer the best results in terms of capturing long-distance relationships.</p>\n",
    "<h5>IV</h5>\n",
    "<p>All tested models seem to capture some real-world knowledge although they make some significant mistakes in this category. Those mistakest showcase greatly that tested models don't have a real understanding of the language as most mistakes made in the real-world knowledge were correct in terms of grammar but completely illogical ex. \"Putin leży pomiędzy Chinami a Rosją.\". In some test cases however I am willing to be lenient since knowledge of certain things isn't an universal property of every human. For example we may ask ourselves if we questioned 100 randomly chosen correspondents, how many of them would point to Mongolia as the country landlocked between China and Russia.</p>\n",
    "<h5>V</h5>\n",
    "<p>The most striking mistakes by test:\n",
    "    Polish cases:\n",
    "    <ul>\n",
    "    <li> Witaj wszystkim! - TwHIN-BERT\n",
    "    <li> Anna od dawna marzyła o ciebie. - TwHIN-BERT\n",
    "    <li> Witaj cie! - xlm roberta\n",
    "    </ul>\n",
    "    Long-range relationships:\n",
    "    <ul>\n",
    "    <li> Marek wrócił do domu i natychmiast wraca na kanapie. - TwHIN-BERT\n",
    "    <li> To drzewo . 50 lat zanim spłonęło. - TwHIN-BERT\n",
    "    <li> Tomek postanowił odwiedzić Marka, ponieważ bardzo ich lubił. - TwHIN-BERT\n",
    "    <li> Marek wrócił do domu i natychmiast siedzi na kanapie. - xlm roberta\n",
    "    <li> Anna wybrała się do sklepu, gdzie kupić mleko. - polish BERT\n",
    "    <li> Tomek postanowił odwiedzić Marka, ponieważ bardzo ją lubił. - polish BERT\n",
    "    </ul>\n",
    "    Real-world knowledge:\n",
    "    <ul>\n",
    "    <li> Tomek studiuje polonistykę na żywo. - TwHIN-BERT\n",
    "    <li> Czas wschodzi o ósmej i zachodzi około szesnastej. - TwHIN-BERT\n",
    "    <li> Nocami wpatrywał się w twarz Saturna. - TwHIN-BERT\n",
    "    <li> Nocami wpatrywał się w oczy Saturna. - TwHIN-BERT\n",
    "    <li> Polska leży pomiędzy Chinami a Rosją. - TwHIN-BERT\n",
    "    <li> Putin leży pomiędzy Chinami a Rosją. - TwHIN-BERT\n",
    "    <li> Polska leży pomiędzy Chinami a Rosją. - xlm roberta\n",
    "    <li> Ukraina leży pomiędzy Chinami a Rosją. - xlm roberta\n",
    "    <li> Nocami wpatrywał się w gwiazdy Saturna. - polish BERT\n",
    "    <li> Polska leży pomiędzy Chinami a Rosją. - polish BERT\n",
    "    </ul>\n",
    "   Worst mistakes were made by the TwHIN-BERT model. At the same time that most striking and numerous mistakes were made in the real-world knowledge category, this best showcases that tested models present little \"understanding\" of the concepts that language communicates and just \"pretend\" to understand the information given in sentences.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b867a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
